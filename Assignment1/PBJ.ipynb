{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Assignment 1\n",
    "This jupyter notebook is meant to be used in conjunction with the full questions in the assignment pdf.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analyses in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of the other cells.\n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Rename the notebook to `<roll_number>.ipynb` and submit ONLY the notebook file on moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Environment setup\n",
    "\n",
    "The following code reads the train and test data (provided along with this template) and outputs the data and labels as numpy arrays. Use these variables in your code.\n",
    "\n",
    "---\n",
    "#### Note on conventions\n",
    "In mathematical notation, the convention is tha data matrices are column-indexed, which means that a input data $x$ has shape $[d, n]$, where $d$ is the number of dimensions and $n$ is the number of data points, respectively.\n",
    "\n",
    "Programming languages have a slightly different convention. Data matrices are of shape $[n, d]$. This has the benefit of being able to access the ith data point as a simple `data[i]`.\n",
    "\n",
    "What this means is that you need to be careful about your handling of matrix dimensions. For example, while the covariance matrix (of shape $[d,d]$) for input data $x$ is calculated as $(x-u)(x-u)^T$, while programming you would do $(x-u)^T(x-u)$ to get the correct output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 784) (1000, 784)\n",
      "(6000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    num_points = len(lines)\n",
    "    dim_points = 28 * 28\n",
    "    data = np.empty((num_points, dim_points))\n",
    "    labels = np.empty(num_points)\n",
    "    \n",
    "    for ind, line in enumerate(lines):\n",
    "        num = line.split(',')\n",
    "        labels[ind] = int(num[0])\n",
    "        data[ind] = [ int(x) for x in num[1:] ]\n",
    "        \n",
    "    return (data, labels)\n",
    "\n",
    "train_data, train_labels = read_data(\"sample_train.csv\")\n",
    "test_data, test_labels = read_data(\"sample_test.csv\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Questions\n",
    "---\n",
    "## 1.3.1 Representation\n",
    "The next code cells, when run, should plot the eigen value spectrum of the covariance matrices corresponding to the mentioned samples. Normalize the eigen value spectrum and only show the first 100 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ca26eff1b2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmy_digit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmy_digit_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_test_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmy_digit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmy_digit_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "# Samples corresponding to the last digit of your roll number (plot a)\n",
    "from numpy import linalg as LA\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "total_test_points = len(train_data)\n",
    "my_digit = 3\n",
    "my_digit_data = []\n",
    "for ind in xrange(total_test_points):\n",
    "    if(train_labels[ind] == my_digit):\n",
    "        my_digit_data.append(train_data[ind])\n",
    "\n",
    "my_cov = np.cov(my_digit_data,rowvar=False)\n",
    "eig_vals ,eig_vecs = LA.eigh(my_cov)\n",
    "max_val = np.amax(eig_vals)\n",
    "norm_eig_vals = eig_vals / max_val\n",
    "\n",
    "top = (norm_eig_vals[-100:])\n",
    "# print(top)\n",
    "plt.bar((np.arange(len(top))),(top))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-508195931313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myour_digit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmy_digit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0myour_digit_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_test_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0myour_digit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0myour_digit_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "# Samples corresponding to the last digit of (your roll number + 1) % 10 (plot b)\n",
    "your_digit = (my_digit + 1) % 10\n",
    "your_digit_data = []\n",
    "for ind in xrange(total_test_points):\n",
    "    if(train_labels[ind] == your_digit):\n",
    "        your_digit_data.append(train_data[ind])\n",
    "\n",
    "my_cov = np.cov(your_digit_data,rowvar=False)\n",
    "eig_vals ,eig_vecs = LA.eigh(my_cov)\n",
    "max_val = np.amax(eig_vals)\n",
    "norm_eig_vals = eig_vals / max_val\n",
    "\n",
    "top = (norm_eig_vals[-100:])\n",
    "# print(top)\n",
    "plt.bar((np.arange(len(top))),(top))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All training data (plot c)\n",
    "my_cov = np.cov(train_data,rowvar=False)\n",
    "eig_vals ,eig_vecs = LA.eigh(my_cov)\n",
    "# print(eig_vals)\n",
    "max_val = np.amax(eig_vals)\n",
    "norm_eig_vals = eig_vals / max_val\n",
    "\n",
    "top = (norm_eig_vals[-100:])\n",
    "# print(top)\n",
    "plt.bar((np.arange(len(top))),(top))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Population must be a sequence or set.  For dicts, use list(d).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c277d5e6acb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrand_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmy_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrowvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0meig_vals\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0meig_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Population must be a sequence or set.  For dicts, use list(d).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Population must be a sequence or set.  For dicts, use list(d)."
     ]
    }
   ],
   "source": [
    "# Randomly selected 50% of the training data (plot d)\n",
    "from random import sample\n",
    "\n",
    "rand_data = sample(train_data,3000)\n",
    "my_cov = np.cov(rand_data,rowvar=False)\n",
    "eig_vals ,eig_vecs = LA.eigh(my_cov)\n",
    "max_val = np.amax(eig_vals)\n",
    "norm_eig_vals = eig_vals / max_val\n",
    "\n",
    "top = (norm_eig_vals[-100:])\n",
    "# print(top)\n",
    "plt.bar((np.arange(len(top))),(top))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.1 Question 1\n",
    "- Are plots a and b different? Why?\n",
    "- Are plots b and c different? Why?\n",
    "- What are the approximate ranks of each plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your answers here (double click to edit)\n",
    "\n",
    "1. Plots a and b are not much different as only a single kind of distribution is there.\n",
    "\n",
    "2. Plots b and c are quite different as c comprises of examples from multiple classes and thus having a wider spectrum.\n",
    "\n",
    "3. Ranks\n",
    "    a-5</br>\n",
    "    b-5</br>\n",
    "    c-15</br>\n",
    "    d-15</br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.1 Question 2\n",
    "- How many possible images could there be?\n",
    "- What percentage is accessible to us as MNIST data?\n",
    "- If we had acces to all the data, how would the eigen value spectrum of the covariance matrix look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your answers here (double click to edit)\n",
    "\n",
    "1. There can be 2^784 images possible.  1.017458e+236\n",
    "\n",
    "2. Approximately 0 percent.\n",
    "\n",
    "3. The eign values will be all large. Since there will be distribution along all the vectors.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 1.3.2 Linear Transformation\n",
    "---\n",
    "### 1.3.2 Question 1\n",
    "How does the eigen spectrum change if the original data was multiplied by an orthonormal matrix? Answer analytically and then also validate experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Analytical answer here (double click to edit)\n",
    "\n",
    "The eign values do not change since the matrix just rotates so the eign vectors change their orientation but the spread remains the same. </br>\n",
    "An Orthonormal matrix preserves angles and length. </br>  \n",
    "\n",
    "https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthonormal-basis/v/lin-alg-orthogonal-matrices-preserve-angles-and-lengths). </br>\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimental validation here.\n",
    "# Multiply your data (train_data) with an orthonormal matrix and plot the\n",
    "# eigen value specturm of the new covariance matrix.\n",
    "\n",
    "# code goes here\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "onm = ortho_group.rvs(dim = 784)\n",
    "\n",
    "my_cov = np.cov(np.matmul(train_data,onm),rowvar=False)\n",
    "eig_vals ,eig_vecs = LA.eigh(my_cov)\n",
    "# print(eig_vals)\n",
    "max_val = np.amax(eig_vals)\n",
    "norm_eig_vals = eig_vals / max_val\n",
    "\n",
    "top = (norm_eig_vals[-100:])\n",
    "# print(top)\n",
    "plt.bar((np.arange(len(top))),(top))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.2 Question 2\n",
    "If  samples  were  multiplied  by  784 Ã— 784  matrix  of rank 1 or 2, (rank deficient matrices), how will the eigen spectrum look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your answer here (double click to edit)\n",
    "\n",
    "Then only the first 1 or 2 columns of the original matrix will be there in the final matrix. So there will be only 2 rows which will contain non-zero values in the co-variance matrix. Thus there will be 1-2 non zero eign values and rest will be zeroes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.2 Question 3\n",
    "Project the original data into the first and second eigenvectors and plot in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de6813d86906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plotting code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrowvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0meig_vals\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0meig_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meig_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting code here\n",
    "my_cov = np.cov(train_data,rowvar=False)\n",
    "eig_vals ,eig_vecs = LA.eigh(my_cov)\n",
    "\n",
    "idx = eig_vals.argsort()[::-1]   \n",
    "eig_vals = eig_vals[idx]\n",
    "eig_vecs = eig_vecs[:,idx]\n",
    "\n",
    "vec1 = eig_vecs[:,0]\n",
    "vec2 = eig_vecs[:,1]\n",
    "\n",
    "x_cors = np.matmul(train_data,vec1)\n",
    "y_cors = np.matmul(train_data,vec2)\n",
    "\n",
    "plt.scatter(x_cors,y_cors)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 1.3.3 Probabilistic View\n",
    "---\n",
    "In this section you will classify the test set by fitting multivariate gaussians on the train set, with different choices for decision boundaries. On running, your code should print the accuracy on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy on the test set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy on the test set using MAP\n",
    "# (assume a reasonable prior and mention it in the comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy using Bayesian pairwise majority voting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy using Simple Perpendicular Bisector majority voting method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.3 Question 4\n",
    "Compare performances and salient observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your analysis here (double click to edit)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 1.3.4 Nearest Neighbour based Tasks and Design\n",
    "---\n",
    "### 1.3.4 Question 1 : NN Classification with various K\n",
    "Implement a KNN classifier and print accuracies on the test set with K=1,3,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.99      0.96       100\n",
      "         1.0       0.80      1.00      0.89       100\n",
      "         2.0       0.99      0.76      0.86       100\n",
      "         3.0       0.93      0.92      0.92       100\n",
      "         4.0       0.92      0.88      0.90       100\n",
      "         5.0       0.96      0.93      0.94       100\n",
      "         6.0       0.92      0.95      0.94       100\n",
      "         7.0       0.88      0.91      0.89       100\n",
      "         8.0       0.96      0.82      0.89       100\n",
      "         9.0       0.85      0.92      0.88       100\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.91      0.91      0.91      1000\n",
      "weighted avg       0.91      0.91      0.91      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Print accuracies with K = 1, 3, 7\n",
    "\n",
    "no_of_test_samples = test_data.shape[0]\n",
    "no_of_dimensions = test_data.shape[1]\n",
    "kone_ans = np.empty(no_of_test_samples)\n",
    "kthree_ans = np.empty(no_of_test_samples)\n",
    "kseven_ans = np.empty(no_of_test_samples)\n",
    "\n",
    "\n",
    "for i in range(0,no_of_test_samples):\n",
    "    row = test_data[i]\n",
    "    \n",
    "    diff_mat = np.square(train_data - row)\n",
    "    diff_arr = np.sum(diff_mat,axis=1).tolist()\n",
    "    diff_arr = np.asarray(diff_arr)\n",
    "    diff_arr = np.sqrt(diff_arr)\n",
    "\n",
    "    idx = diff_arr.argsort()[::1]   \n",
    "    diff_arr = diff_arr[idx]\n",
    "    sorted_labels = train_labels[idx]\n",
    "\n",
    "    ## for k = 1.\n",
    "    kone_ans[i] = sorted_labels[0]  \n",
    "    \n",
    "    ## for k=3\n",
    "    dict3 = dict((cate,0) for cate in range(0,10))\n",
    "    for ii in range(0,3):\n",
    "        dict3[sorted_labels[ii]] = dict3[sorted_labels[ii]] + 1\n",
    "\n",
    "    sorted_three = sorted(dict3,key=dict3.get,reverse=True)    \n",
    "    kthree_ans[i] = int(sorted_three[0])\n",
    "        \n",
    "    ## for k=7\n",
    "    dict7 = dict((cate,0) for cate in range(0,10))\n",
    "    for ii in range(0,7):\n",
    "        dict7[sorted_labels[ii]] = dict7[sorted_labels[ii]] + 1\n",
    "    \n",
    "    sorted_seven = sorted(dict7,key=dict7.get,reverse=True)    \n",
    "    kseven_ans[i] = int(sorted_seven[0])  \n",
    "\n",
    "## k = 1\n",
    "count = 0\n",
    "for ind in range(0,no_of_test_samples):\n",
    "    \n",
    "    if (kone_ans[ind] == test_labels[ind]):\n",
    "        count = count + 1\n",
    "\n",
    "# print(count)\n",
    "accuracy_kone = (count/float(no_of_test_samples)) * 100\n",
    "print(\"accuracy with k = 1 :\",accuracy_kone)\n",
    "\n",
    "## k = 3\n",
    "count = 0\n",
    "for ind in range(0,no_of_test_samples):\n",
    "    \n",
    "    if (kthree_ans[ind] == test_labels[ind]):\n",
    "        count = count + 1\n",
    "\n",
    "# print(count)\n",
    "accuracy_kthree = (count/float(no_of_test_samples)) * 100\n",
    "print(\"accuracy with k = 3 :\",accuracy_kthree)\n",
    "\n",
    "\n",
    "## k = 7\n",
    "count = 0\n",
    "for ind in range(0,no_of_test_samples):\n",
    "    \n",
    "    if (kseven_ans[ind] == test_labels[ind]):\n",
    "        count = count + 1\n",
    "\n",
    "# print(count)\n",
    "accuracy_kseven = (count/float(no_of_test_samples)) * 100\n",
    "print(\"accuracy with k = 7 :\",accuracy_kseven)  \n",
    "    \n",
    "## Inbuilt Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=11)  \n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "y_pred = classifier.predict(test_data) \n",
    "\n",
    "print(classification_report(test_labels, y_pred))\n",
    "\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.4 Question 1 continued\n",
    "- Why / why not are the accuracies the same?\n",
    "- How do we identify the best K? Suggest a computational procedure with a logical explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your analysis here (double click to edit)\n",
    "\n",
    "1. The accuracies are not same since the class may vary as k increases/decreases.\n",
    "\n",
    "2. K should not be very small otherwise there will be significant contribution of noise, also when k increases largely the computational overload increases and also there is a chance that model overfits. For identifying the best <b> K </b> , we split the training data into a validation set and traning set (8:2) ratio. Then test for different k values on the validation set and finally take that k , which provides the best accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.4 Question 2 :  Reverse NN based outlier detection\n",
    "A sample can be thought of as an outlier is it is NOT in the nearest neighbour set of anybody else. Expand this idea into an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 784)\n"
     ]
    }
   ],
   "source": [
    "# This cell reads mixed data containing both MNIST digits and English characters.\n",
    "# The labels for this mixed data are random and are hence ignored.\n",
    "mixed_data, _ = read_data(\"outliers.csv\")\n",
    "print(mixed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.4 Question 3 : NN for regression\n",
    "Assume that each classID in the train set corresponds to a neatness score as:\n",
    "$$ neatness = \\frac{classID}{10} $$\n",
    "\n",
    "---\n",
    "Assume we had to predict the neatness score for each test sample using NN based techiniques on the train set. Describe the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your algorithm here (double click to edit)\n",
    "\n",
    "1. Firstly all the train and test labels will have to be assigned a neatness score. And each class will have a range\n",
    "2. We take the test point and compute distances from all the points and sort it.\n",
    "3. We now pick the k nearest neighbours.\n",
    "4. Now we divide the label value of each of the knn by their corresponding distances to have a weighted distribution.\n",
    "5. We sum up the values and find the label of the test sample.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### 1.3.4 Question 3 continued\n",
    "Validate your algorithm on the test set. This code should print mean absolute error on the test set, using the train set for NN based regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71131 0.20752 0.10499 0.      0.58482 0.11839 0.59215 0.90797 0.50267\n",
      " 1.00958 0.      0.6077  0.951   0.      0.10983 0.50827 0.75987 0.73685\n",
      " 0.32079 0.42103 0.96218 0.64068 0.61602 0.5187  0.98472 0.      0.73737\n",
      " 0.4103  0.      0.10538 0.31562 0.10448 0.30476 0.40809 0.77137 0.20419\n",
      " 0.71204 0.1183  0.24129 0.10543 0.1087  0.73806 0.40819 0.10192 0.31663\n",
      " 0.50735 0.10943 0.20987 0.40922 0.45418 0.60776 0.31417 0.53204 0.54625\n",
      " 0.63009 0.      0.41442 0.12561 1.02858 0.52888 0.72649 0.40238 0.75967\n",
      " 0.3254  0.70725 0.60034 0.70724 0.41005 0.33644 0.      0.7149  0.\n",
      " 0.13694 0.83874 0.1137  0.71058 0.31415 0.57058 0.93498 0.73181 0.80013\n",
      " 0.63336 0.2059  0.75116 0.84373 0.4081  0.71701 0.30337 0.63969 0.10221\n",
      " 0.36941 0.61888 0.58369 0.30473 0.10726 0.41372 0.10738 0.73456 0.6442\n",
      " 0.93192 0.63238 0.      0.51174 0.42848 0.90794 0.95101 0.20072 0.11327\n",
      " 0.9077  0.6039  0.66818 0.36857 0.30915 0.98879 0.74623 0.91651 0.42521\n",
      " 0.40814 0.9145  0.57336 0.52051 0.40889 0.73758 0.66007 0.81111 0.98172\n",
      " 0.      0.56896 0.81433 0.53878 0.61648 0.62804 0.50117 0.70264 0.81088\n",
      " 0.10136 0.      0.10729 0.61086 0.42632 0.60982 0.78563 0.32178 0.10989\n",
      " 0.73598 0.11336 0.84635 0.21495 0.      0.13383 0.91868 0.87981 0.50425\n",
      " 0.51917 0.11081 0.52898 0.65274 0.      0.30714 0.211   0.40614 0.62543\n",
      " 0.51461 0.43241 0.63436 0.51368 0.41489 0.51319 0.10197 0.43908 0.42572\n",
      " 0.76455 0.20482 0.36919 0.21168 0.58359 0.12396 0.82406 0.1136  0.80373\n",
      " 0.1078  0.80604 0.56015 0.      0.64994 0.98227 0.21342 0.60565 0.\n",
      " 0.10792 0.10036 0.11612 0.      0.86358 0.      0.16685 0.11938 0.62141\n",
      " 0.42315 0.23632 0.30487 0.61753 0.10974 0.10724 0.11027 0.33835 0.935\n",
      " 0.40203 0.2126  0.90247 0.41394 0.52903 0.97519 0.31192 0.93762 0.\n",
      " 0.3181  0.6014  0.51623 0.44586 0.72287 0.21392 0.20241 0.7196  0.11036\n",
      " 0.21061 0.8484  0.41096 0.11312 0.83186 0.37303 0.31722 0.81812 0.77102\n",
      " 0.73512 0.90418 0.20383 0.20217 0.42412 0.10246 0.52319 0.71853 0.8993\n",
      " 0.75118 0.36972 0.44047 0.      0.60704 0.46279 0.18111 0.73907 0.10742\n",
      " 0.99488 0.51066 0.73774 0.72166 0.20863 0.67703 0.20664 0.4898  0.81493\n",
      " 0.50761 0.78214 0.73308 0.57983 0.1081  0.40327 0.10335 0.48099 0.\n",
      " 0.30611 0.      0.10219 0.9313  0.96459 0.41834 0.10246 0.81797 0.20424\n",
      " 0.10676 0.21696 0.9327  0.72155 0.51259 0.91573 0.2021  0.61693 0.577\n",
      " 0.10907 0.53044 0.91183 0.20541 0.90972 0.20629 0.      0.42235 0.\n",
      " 0.      0.20901 0.83466 0.10067 0.74716 0.10896 0.37271 0.42225 0.\n",
      " 0.21713 0.74281 0.41622 0.31377 0.30256 0.      0.      0.4068  0.10505\n",
      " 0.95703 0.69141 0.50636 0.06678 0.62397 0.51921 0.71932 0.96482 0.32577\n",
      " 0.36739 0.43382 0.47634 0.      0.76704 0.12395 0.10595 0.25195 0.10735\n",
      " 0.50418 0.30206 0.46879 0.95496 0.7153  0.6068  0.66034 0.49146 0.58441\n",
      " 0.10234 0.30813 0.80766 0.10293 0.      0.53537 0.10378 0.8445  0.10351\n",
      " 0.51914 0.52378 0.63998 0.1027  0.82693 0.51906 0.10375 0.52317 0.74424\n",
      " 0.41191 0.63493 0.37051 0.20487 0.55467 0.      0.60719 0.7217  0.60695\n",
      " 0.30657 0.7268  0.21183 0.      0.82116 0.80485 0.5338  0.58444 0.10589\n",
      " 0.10508 0.42449 0.      0.72108 0.30926 0.77075 0.6179  0.10143 0.61826\n",
      " 0.2088  0.10707 0.74316 0.21125 0.84671 0.60821 0.10268 0.92013 0.5016\n",
      " 0.20541 0.51429 0.42243 0.41766 0.20092 0.84411 0.31931 0.66031 0.16913\n",
      " 0.41043 0.55664 0.      0.30636 0.10259 0.72495 0.72275 0.37273 0.75534\n",
      " 1.12996 0.76261 0.10588 0.91657 0.21647 0.11097 0.40955 0.21032 0.95161\n",
      " 0.20695 0.      0.41085 1.02054 0.10391 0.40778 0.81582 0.10213 0.84659\n",
      " 0.58273 0.51569 0.91188 0.78598 0.8844  0.30729 0.73751 0.62921 0.\n",
      " 0.      0.30883 0.      0.41834 0.      0.63242 0.42008 0.88636 0.44576\n",
      " 0.30558 0.32914 0.21579 0.31807 0.88156 0.10743 0.18875 0.62144 0.85427\n",
      " 0.      0.66986 0.60976 0.63192 0.6295  0.70454 0.86832 0.85796 0.22007\n",
      " 0.71992 0.51494 0.81373 0.93708 0.60693 0.11168 0.82925 0.40144 0.10484\n",
      " 0.20542 0.65273 0.67651 0.11806 1.00956 0.73477 0.50837 0.43147 0.\n",
      " 0.81207 0.95476 0.83764 0.10862 0.      0.44698 0.61347 0.31891 0.77501\n",
      " 0.06738 0.96429 0.41269 0.      0.60711 0.31804 0.94944 0.53128 0.20246\n",
      " 0.10417 0.31475 0.10455 0.47655 0.61882 0.51094 0.71066 0.68711 0.2196\n",
      " 0.20052 0.70009 0.31616 0.20256 0.62026 0.50843 0.42675 0.86851 0.73897\n",
      " 0.82952 0.11573 0.3695  0.      0.30167 0.50301 0.30807 0.10874 0.91172\n",
      " 0.42199 0.41139 0.44351 0.63027 0.41063 0.20817 0.10653 0.87165 0.20492\n",
      " 0.5108  0.41815 0.3348  0.87106 0.42316 0.      0.      0.61171 0.30898\n",
      " 0.21598 0.73796 0.17004 0.2     0.77441 0.71797 0.42683 0.41876 0.82634\n",
      " 0.90511 0.65289 0.9963  0.      0.76809 0.83454 0.      0.57276 0.61135\n",
      " 0.      0.61575 0.3041  0.64268 0.78323 0.81863 0.34998 0.31755 0.98604\n",
      " 0.31774 0.30544 0.44928 0.81728 0.80802 0.      0.37398 0.71681 0.10826\n",
      " 0.71858 0.      0.61627 0.55276 0.42138 0.31501 0.48428 0.      0.9172\n",
      " 0.60371 0.32877 0.7074  0.      0.92996 0.94271 0.6356  0.83476 0.64843\n",
      " 0.514   0.53323 0.78648 0.64967 0.65187 0.      0.20445 0.41363 0.\n",
      " 0.2233  0.14678 0.30233 0.10209 0.94139 0.7395  0.50928 0.3431  0.\n",
      " 0.81765 0.41174 0.62616 0.20935 0.60624 0.76781 0.75488 0.73289 0.22318\n",
      " 0.92207 0.80762 0.22044 0.21772 1.01806 0.10064 0.72706 0.32172 0.57133\n",
      " 0.93249 0.10407 0.81611 0.      0.21131 0.      0.63282 0.55383 0.10381\n",
      " 0.30256 0.73192 0.62741 0.7326  0.10163 0.21145 0.34084 0.65029 0.\n",
      " 0.30876 0.78339 0.71381 0.41769 0.      1.00691 0.10468 0.80864 0.60831\n",
      " 0.8022  0.10641 0.74796 0.30133 0.6242  0.96096 0.1071  0.93737 0.50678\n",
      " 0.10088 0.71701 0.30272 0.95511 0.77387 0.62252 0.94249 0.10778 0.30678\n",
      " 0.16744 0.81962 0.33355 0.32651 0.62867 0.96235 0.21845 0.50872 0.50905\n",
      " 0.81394 0.51071 0.10499 0.10246 0.42286 0.41266 0.33143 0.1035  0.\n",
      " 0.7184  0.74725 0.      0.83525 1.02191 0.96473 0.42761 0.80881 0.44323\n",
      " 0.52291 0.59996 0.      0.73582 0.20766 0.10298 0.2     0.81492 0.42918\n",
      " 0.62316 0.      0.419   0.      0.63194 0.11021 0.57196 0.47556 0.20125\n",
      " 0.61933 0.80067 0.20087 0.63192 0.92461 0.30585 0.10879 0.40913 0.62608\n",
      " 0.40363 0.51566 0.92702 0.2047  0.      0.64435 0.20565 0.10593 0.75952\n",
      " 0.3126  0.46873 0.10666 0.      0.43702 0.41144 0.31218 0.11466 0.10451\n",
      " 0.80025 0.43894 0.94567 0.95194 0.74551 0.82456 0.5961  0.      0.215\n",
      " 0.4143  0.60219 0.10505 0.1095  0.63834 0.40189 0.78151 0.11042 0.91692\n",
      " 0.48401 0.21928 0.42916 0.11394 0.51779 0.57958 0.30583 0.63681 0.30678\n",
      " 0.10826 0.41461 0.44371 0.63743 0.66464 0.90977 0.43084 0.10436 0.80712\n",
      " 0.31382 0.82336 0.      0.60332 0.21512 0.2357  0.11502 0.20997 0.52431\n",
      " 0.31077 0.41675 0.40154 0.      0.85762 0.86103 0.30282 0.31258 0.10073\n",
      " 0.54115 0.30969 0.50123 0.87598 0.61144 0.30243 0.20114 0.63237 0.30278\n",
      " 0.61174 0.      0.71836 0.20231 0.7415  0.58118 0.2184  0.57944 0.35153\n",
      " 0.73741 0.97576 0.60058 0.20075 0.57616 0.37421 0.79049 0.72326 0.4181\n",
      " 0.34265 0.      0.70575 0.30656 0.30935 0.      0.7749  0.71482 0.\n",
      " 0.31064 0.52302 0.52499 0.20155 0.76088 0.64773 0.62619 0.92087 0.20098\n",
      " 0.81192 0.30547 0.61901 0.20566 0.20659 0.54148 0.63432 0.      0.81839\n",
      " 0.20677 0.92175 0.20133 0.72846 0.50732 0.70953 0.70091 0.81114 0.60789\n",
      " 0.50106 0.40167 0.      0.62441 0.68256 0.30646 0.20727 0.39344 0.20476\n",
      " 0.95481 0.48125 0.      0.      0.50225 0.70636 0.95438 0.43285 0.6487\n",
      " 0.      0.94985 0.74827 0.76454 0.31222 0.96493 0.87401 0.84634 0.79295\n",
      " 0.32398 0.30646 0.30607 0.94495 0.10054 0.      0.31232 0.50945 0.5307\n",
      " 0.76025 0.65376 0.5959  0.83546 0.61358 0.61258 0.7405  0.91429 0.74537\n",
      " 0.67532 0.6032  0.78582 0.54677 0.82348 0.82181 0.70655 0.20131 0.91907\n",
      " 0.51405 0.      0.64673 0.50503 0.      0.83202 0.67372 0.70525 0.61276\n",
      " 0.9373  0.      0.      0.81635 1.00227 0.93591 0.      0.30336 0.93379\n",
      " 0.73914 0.84424 0.      0.9173  0.51378 0.61851 0.87269 0.50368 0.71609\n",
      " 0.82021 0.50204 0.81016 0.      0.64599 0.53613 0.60395 0.98781 0.56394\n",
      " 0.      0.91789 0.81881 0.969   0.      0.8802  0.50573 0.52923 0.81129\n",
      " 0.65674 0.51836 0.      0.65148 0.50996 0.33419 0.80914 0.      0.61763\n",
      " 0.59876 0.53134 0.66051 0.71203 0.52837 0.53267 0.84988 0.      0.6097\n",
      " 0.      0.65028 0.63356 0.      0.62564 0.      0.      0.      0.\n",
      " 0.     ]\n",
      "mean absolute error is: 0.05636793566710551\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "train_labels_reg = train_labels / float(10)\n",
    "test_labels_reg = test_labels / float(10)\n",
    "\n",
    "no_of_test_samples = test_data.shape[0]\n",
    "no_of_dimensions = test_data.shape[1]\n",
    "k_ans = np.empty(no_of_test_samples)\n",
    "\n",
    "\n",
    "for i in range(0,no_of_test_samples):\n",
    "    row = test_data[i]\n",
    "    \n",
    "    ## distances are calulcated and sorted.\n",
    "    diff_mat = np.square(train_data - row)\n",
    "    diff_arr = np.sum(diff_mat,axis=1).tolist()\n",
    "    diff_arr = np.asarray(diff_arr)\n",
    "    diff_arr = np.sqrt(diff_arr)\n",
    "\n",
    "    idx = diff_arr.argsort()[::1]   \n",
    "    diff_arr = diff_arr[idx]\n",
    "    sorted_labels = train_labels_reg[idx]\n",
    "\n",
    "    ## for any k. compute the neatness of the sample.\n",
    "    k = 3\n",
    "    denom = diff_arr[k-1]\n",
    "    neatness = 0\n",
    "    for ii in range(0,k):\n",
    "        diff_arr[ii] = diff_arr[ii] / float(denom)\n",
    "        neatness = neatness + (sorted_labels[ii] / float(diff_arr[ii]))\n",
    "        \n",
    "    neatness = neatness / float(k)\n",
    "    \n",
    "    k_ans[i] = neatness\n",
    "\n",
    "# print(k_ans)\n",
    "error = 0\n",
    "for i in range(0,no_of_test_samples):\n",
    "    error = error + abs(k_ans[i] - test_labels_reg[i])\n",
    "\n",
    "\n",
    "error = error / float(no_of_test_samples)\n",
    "\n",
    "print(\"mean absolute error is:\",error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "---\n",
    "# FOLLOW THE SUBMISSION INSTRUCTIONS\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
